<img src="https://i.ibb.co/DtHQ3FG/802x265-Logo-GT.png" width="500">

## Grupo Turing
# Random Forest (Regressão)

- **Resumo**:
Este método supervisionado de regressão é baseado no método de bagging (mais detalhado na seção deste assunto em Ensemble Learning), onde decision trees são executadas em paralelo, sem interações entre si. No caso de um modelo de classificação, o resultado final é a moda dos resultados fornecidos pelas árvores, enquanto, para a regressão, o resultado é a média da previsão de cada uma das árvores individuais. 


<img src="https://imgur.com/zHAF3YN.png" width="500">

A vantagem de se utilizar uma random forest ao invés de somente uma Decision Tree é que os hiperparâmetros desta garantem que o modelo não depende muito de uma só feature, além de reduzir o overfitting, pois cada uma das decision trees usa uma parte aleatória do dataset, reduzindo a correlação entre elas. Portanto, essas Decisions Trees, ao se unirem, criam um modelo muito mais balanceado.

Veja também:
* [Decision Tree Regressor]()
* [Random Forest Classifier]()
* [Bagging]()


- **Links Úteis**:
    - [Texto do Turing Talks (Classifier)](https://medium.com/turing-talks/turing-talks-18-modelos-de-predi%C3https://www.quora.com/How-does-random-forest-work-for-regression-1%A7%C3%A3o-random-forest-cfc91cd8e524
)
    - [Random Forest Regression](https://towardsdatascience.com/random-forest-and-its-implementation-71824ced454f)
    - [A Beginners Guide to Random Forest Regression](https://medium.com/datadriveninvestor/random-forest-regression-9871bc9a25eb)
    - [How does random forest work for regression?](https://www.quora.com/How-does-random-forest-work-for-regression-1)

---
**Grupo Turing**  
Grupo de Extensão da Universidade de São Paulo (USP)

[Email](mailto:turing.usp@gmail.com)   
[Facebook](https://www.facebook.com/grupoturing.usp)  
[Medium](https://www.medium.com/turing-talks)  
[LinkedIn](https://www.linkedin.com/company/grupo-turing)

https://www.quora.com/How-does-random-forest-work-for-regression-1